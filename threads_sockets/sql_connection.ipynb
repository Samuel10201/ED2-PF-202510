{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf4dc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mysql.connector import connect, errorcode, Error, pooling\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from os import environ\n",
    "from fastavro import writer, parse_schema\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from queue import Queue\n",
    "import time\n",
    "import math\n",
    "import threading\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f49e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "📦 Procesando tamaño: 100 registros\n",
    "[CSV] Tamaño: 100 → Promedio: 0.0023 s\n",
    "[JSON] Tamaño: 100 → Promedio: 0.0061 s\n",
    "[PARQUET] Tamaño: 100 → Promedio: 0.0062 s\n",
    "[AVRO] Tamaño: 100 → Promedio: 0.0230 s\n",
    "\n",
    "📦 Procesando tamaño: 34579 registros\n",
    "[JSON] Tamaño: 34579 → Promedio: 0.3755 s\n",
    "[CSV] Tamaño: 34579 → Promedio: 0.2373 s\n",
    "[PARQUET] Tamaño: 34579 → Promedio: 0.2062 s\n",
    "[AVRO] Tamaño: 34579 → Promedio: 0.7423 s\n",
    "\n",
    "📦 Procesando tamaño: 69058 registros\n",
    "[CSV] Tamaño: 69058 → Promedio: 0.6223 s\n",
    "[JSON] Tamaño: 69058 → Promedio: 0.9655 s\n",
    "[PARQUET] Tamaño: 69058 → Promedio: 0.5697 s\n",
    "[AVRO] Tamaño: 69058 → Promedio: 1.5891 s\n",
    "\n",
    "📦 Procesando tamaño: 103537 registros\n",
    "[CSV] Tamaño: 103537 → Promedio: 0.8871 s\n",
    "[JSON] Tamaño: 103537 → Promedio: 1.3955 s\n",
    "[PARQUET] Tamaño: 103537 → Promedio: 0.8876 s\n",
    "[AVRO] Tamaño: 103537 → Promedio: 2.8083 s\n",
    "\n",
    "📦 Procesando tamaño: 138017 registros\n",
    "[CSV] Tamaño: 138017 → Promedio: 1.3226 s\n",
    "[PARQUET] Tamaño: 138017 → Promedio: 1.2968 s\n",
    "[JSON] Tamaño: 138017 → Promedio: 2.0345 s\n",
    "[AVRO] Tamaño: 138017 → Promedio: 3.5155 s\n",
    "\n",
    "📦 Procesando tamaño: 172496 registros\n",
    "[CSV] Tamaño: 172496 → Promedio: 1.5285 s\n",
    "[PARQUET] Tamaño: 172496 → Promedio: 1.5644 s\n",
    "[JSON] Tamaño: 172496 → Promedio: 2.3641 s\n",
    "[AVRO] Tamaño: 172496 → Promedio: 4.8738 s\n",
    "\n",
    "📦 Procesando tamaño: 206975 registros\n",
    "[CSV] Tamaño: 206975 → Promedio: 1.8668 s\n",
    "[PARQUET] Tamaño: 206975 → Promedio: 1.8512 s\n",
    "[JSON] Tamaño: 206975 → Promedio: 2.8255 s\n",
    "[AVRO] Tamaño: 206975 → Promedio: 5.7157 s\n",
    "\n",
    "📦 Procesando tamaño: 241455 registros\n",
    "[CSV] Tamaño: 241455 → Promedio: 2.1849 s\n",
    "[PARQUET] Tamaño: 241455 → Promedio: 2.2509 s\n",
    "[JSON] Tamaño: 241455 → Promedio: 3.2904 s\n",
    "[AVRO] Tamaño: 241455 → Promedio: 6.5960 s\n",
    "\n",
    "📦 Procesando tamaño: 275934 registros\n",
    "[CSV] Tamaño: 275934 → Promedio: 2.7163 s\n",
    "[PARQUET] Tamaño: 275934 → Promedio: 2.5522 s\n",
    "[JSON] Tamaño: 275934 → Promedio: 3.9516 s\n",
    "[AVRO] Tamaño: 275934 → Promedio: 8.5093 s\n",
    "\n",
    "📦 Procesando tamaño: 310413 registros\n",
    "[PARQUET] Tamaño: 310413 → Promedio: 3.0392 s\n",
    "[CSV] Tamaño: 310413 → Promedio: 3.8460 s\n",
    "[JSON] Tamaño: 310413 → Promedio: 5.5881 s\n",
    "[AVRO] Tamaño: 310413 → Promedio: 10.8727 s\n",
    "\n",
    "📦 Procesando tamaño: 344893 registros\n",
    "[PARQUET] Tamaño: 344893 → Promedio: 3.4305 s\n",
    "[CSV] Tamaño: 344893 → Promedio: 4.2786 s\n",
    "[JSON] Tamaño: 344893 → Promedio: 6.2082 s\n",
    "[AVRO] Tamaño: 344893 → Promedio: 12.8065 s\n",
    "\n",
    "📦 Procesando tamaño: 379372 registros\n",
    "[PARQUET] Tamaño: 379372 → Promedio: 3.8131 s\n",
    "[CSV] Tamaño: 379372 → Promedio: 4.7316 s\n",
    "[JSON] Tamaño: 379372 → Promedio: 6.8617 s\n",
    "[AVRO] Tamaño: 379372 → Promedio: 13.1572 s\n",
    "\n",
    "📦 Procesando tamaño: 413851 registros\n",
    "[PARQUET] Tamaño: 413851 → Promedio: 4.1657 s\n",
    "[CSV] Tamaño: 413851 → Promedio: 5.1102 s\n",
    "[JSON] Tamaño: 413851 → Promedio: 7.2876 s\n",
    "[AVRO] Tamaño: 413851 → Promedio: 13.9809 s\n",
    "\n",
    "📦 Procesando tamaño: 448331 registros\n",
    "[PARQUET] Tamaño: 448331 → Promedio: 4.6001 s\n",
    "[CSV] Tamaño: 448331 → Promedio: 5.6925 s\n",
    "[JSON] Tamaño: 448331 → Promedio: 8.1979 s\n",
    "[AVRO] Tamaño: 448331 → Promedio: 15.2455 s\n",
    "\n",
    "📦 Procesando tamaño: 482810 registros\n",
    "[PARQUET] Tamaño: 482810 → Promedio: 4.8790 s\n",
    "[CSV] Tamaño: 482810 → Promedio: 5.8948 s\n",
    "[JSON] Tamaño: 482810 → Promedio: 8.4626 s\n",
    "[AVRO] Tamaño: 482810 → Promedio: 16.4423 s\n",
    "\n",
    "📦 Procesando tamaño: 517289 registros\n",
    "[PARQUET] Tamaño: 517289 → Promedio: 5.2711 s\n",
    "[CSV] Tamaño: 517289 → Promedio: 6.3859 s\n",
    "[JSON] Tamaño: 517289 → Promedio: 9.2241 s\n",
    "[AVRO] Tamaño: 517289 → Promedio: 17.1507 s\n",
    "\n",
    "📦 Procesando tamaño: 551768 registros\n",
    "[CSV] Tamaño: 551768 → Promedio: 5.8654 s\n",
    "[PARQUET] Tamaño: 551768 → Promedio: 5.6929 s\n",
    "[JSON] Tamaño: 551768 → Promedio: 8.3892 s\n",
    "[AVRO] Tamaño: 551768 → Promedio: 15.9237 s\n",
    "\n",
    "📦 Procesando tamaño: 586248 registros\n",
    "[CSV] Tamaño: 586248 → Promedio: 5.6365 s\n",
    "[PARQUET] Tamaño: 586248 → Promedio: 5.6605 s\n",
    "[JSON] Tamaño: 586248 → Promedio: 8.4472 s\n",
    "[AVRO] Tamaño: 586248 → Promedio: 16.7527 s\n",
    "\n",
    "📦 Procesando tamaño: 620727 registros\n",
    "[PARQUET] Tamaño: 620727 → Promedio: 6.5190 s\n",
    "[CSV] Tamaño: 620727 → Promedio: 7.9272 s\n",
    "[JSON] Tamaño: 620727 → Promedio: 11.3205 s\n",
    "[AVRO] Tamaño: 620727 → Promedio: 22.1795 s\n",
    "\n",
    "📦 Procesando tamaño: 655206 registros\n",
    "[PARQUET] Tamaño: 655206 → Promedio: 6.7563 s\n",
    "[CSV] Tamaño: 655206 → Promedio: 8.0916 s\n",
    "[JSON] Tamaño: 655206 → Promedio: 12.1827 s\n",
    "[AVRO] Tamaño: 655206 → Promedio: 22.7885 s\n",
    "\n",
    "📦 Procesando tamaño: 689686 registros\n",
    "[PARQUET] Tamaño: 689686 → Promedio: 7.2980 s\n",
    "[CSV] Tamaño: 689686 → Promedio: 8.9304 s\n",
    "[JSON] Tamaño: 689686 → Promedio: 13.3323 s\n",
    "[AVRO] Tamaño: 689686 → Promedio: 23.9070 s\n",
    "\n",
    "📦 Procesando tamaño: 724165 registros\n",
    "[PARQUET] Tamaño: 724165 → Promedio: 7.6799 s\n",
    "[CSV] Tamaño: 724165 → Promedio: 9.4313 s\n",
    "[JSON] Tamaño: 724165 → Promedio: 14.0007 s\n",
    "[AVRO] Tamaño: 724165 → Promedio: 23.3296 s\n",
    "\n",
    "📦 Procesando tamaño: 758644 registros\n",
    "[CSV] Tamaño: 758644 → Promedio: 7.7140 s\n",
    "[PARQUET] Tamaño: 758644 → Promedio: 7.7135 s\n",
    "[JSON] Tamaño: 758644 → Promedio: 11.7740 s\n",
    "[AVRO] Tamaño: 758644 → Promedio: 22.1849 s\n",
    "\n",
    "📦 Procesando tamaño: 793124 registros\n",
    "[CSV] Tamaño: 793124 → Promedio: 7.9410 s\n",
    "[PARQUET] Tamaño: 793124 → Promedio: 7.8119 s\n",
    "[JSON] Tamaño: 793124 → Promedio: 12.0255 s\n",
    "[AVRO] Tamaño: 793124 → Promedio: 23.8773 s\n",
    "\n",
    "📦 Procesando tamaño: 827603 registros\n",
    "[CSV] Tamaño: 827603 → Promedio: 11.0065 s\n",
    "[PARQUET] Tamaño: 827603 → Promedio: 9.5260 s\n",
    "[JSON] Tamaño: 827603 → Promedio: 16.0385 s\n",
    "[AVRO] Tamaño: 827603 → Promedio: 26.8138 s\n",
    "\n",
    "📦 Procesando tamaño: 862082 registros\n",
    "[CSV] Tamaño: 862082 → Promedio: 8.9499 s\n",
    "[PARQUET] Tamaño: 862082 → Promedio: 8.6446 s\n",
    "[JSON] Tamaño: 862082 → Promedio: 13.5594 s\n",
    "[AVRO] Tamaño: 862082 → Promedio: 27.6828 s\n",
    "\n",
    "📦 Procesando tamaño: 896562 registros\n",
    "[PARQUET] Tamaño: 896562 → Promedio: 10.4207 s\n",
    "[CSV] Tamaño: 896562 → Promedio: 12.6093 s\n",
    "[JSON] Tamaño: 896562 → Promedio: 18.3747 s\n",
    "[AVRO] Tamaño: 896562 → Promedio: 30.8139 s\n",
    "\n",
    "📦 Procesando tamaño: 931041 registros\n",
    "[PARQUET] Tamaño: 931041 → Promedio: 11.0779 s\n",
    "[CSV] Tamaño: 931041 → Promedio: 13.1513 s\n",
    "[JSON] Tamaño: 931041 → Promedio: 19.1337 s\n",
    "[AVRO] Tamaño: 931041 → Promedio: 31.5847 s\n",
    "\n",
    "📦 Procesando tamaño: 965520 registros\n",
    "[PARQUET] Tamaño: 965520 → Promedio: 11.7487 s\n",
    "[CSV] Tamaño: 965520 → Promedio: 14.1708 s\n",
    "[JSON] Tamaño: 965520 → Promedio: 20.5094 s\n",
    "[AVRO] Tamaño: 965520 → Promedio: 32.8019 s\n",
    "\n",
    "📦 Procesando tamaño: 1000000 registros\n",
    "[CSV] Tamaño: 1000000 → Promedio: 11.6296 s\n",
    "[PARQUET] Tamaño: 1000000 → Promedio: 11.0940 s\n",
    "[JSON] Tamaño: 1000000 → Promedio: 17.3251 s\n",
    "[AVRO] Tamaño: 1000000 → Promedio: 30.5482 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f571b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"user\": environ['DATABASE_USERNAME'],\n",
    "    \"password\": environ['DATABASE_PASSWORD'],\n",
    "    \"host\": environ['DATABASE_HOST'],\n",
    "    \"database\": environ['DATABASE_NAME'],\n",
    "    \"charset\": 'utf8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e372b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection():\n",
    "    try:\n",
    "        print(\"Connecting to the database...\\n config: \", config)\n",
    "        return connect(**config)\n",
    "    except Error as err:\n",
    "        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with your user name or password\")\n",
    "        elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "        else:\n",
    "            print(err)\n",
    "        return None\n",
    "    \n",
    "def get_data(connection: connect, query: str):\n",
    "    my_cursor = connection.cursor()\n",
    "    my_cursor.execute(query)\n",
    "    data = my_cursor.fetchall()\n",
    "    my_cursor.close()\n",
    "    return data\n",
    "    \n",
    "\n",
    "def export_to_avro(data, columns, filename):\n",
    "    # Definición del esquema AVRO con todos los campos como cadenas\n",
    "    schema = {\n",
    "        \"doc\": \"Ventas record\",\n",
    "        \"name\": \"Venta\",\n",
    "        \"namespace\": \"un.database\",\n",
    "        \"type\": \"record\",\n",
    "        \"fields\": [{\"name\": col, \"type\": \"string\"} for col in columns]\n",
    "    }\n",
    "\n",
    "    parsed_schema = parse_schema(schema)\n",
    "\n",
    "    # Convertir cada fila a un diccionario con los nombres de columnas como claves\n",
    "    records = [dict(zip(columns, map(str, row))) for row in data]\n",
    "\n",
    "    # Escribir el archivo AVRO\n",
    "    with open(filename, 'wb') as out_file:\n",
    "        writer(out_file, parsed_schema, records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2396b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtención de datos de la base de datos indicando el inicio de la extracción y la cantidad de datos a obtener. Esto es lo que hará cada hilo\n",
    "def get_data_batch(offset, limit, queue):\n",
    "    cnx = get_connection()\n",
    "    if not cnx:\n",
    "        return\n",
    "    cursor = cnx.cursor()\n",
    "    query = f\"SELECT * FROM UN.VENTAS LIMIT {limit} OFFSET {offset}\"\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    queue.put(data)\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "       \n",
    "def extract_all_data_parallel(batch_size, num_threads):\n",
    "    # Paso 1: Obtener el total de registros de la tabla\n",
    "    cnx = get_connection()\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM UN.VENTAS\")\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "\n",
    "    # Paso 2: Calcular desde dónde debe empezar cada lote de datos\n",
    "    offsets = []\n",
    "    current_offset = 0\n",
    "    while current_offset < total_rows:\n",
    "        offsets.append(current_offset)\n",
    "        current_offset += batch_size\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    # Paso 3: Crear un grupo de hilos (máximo num_threads a la vez)\n",
    "    print(f\"Enviando {len(offsets)} tareas al pool\")\n",
    "    inicio = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for offset in offsets:\n",
    "            print(f\"Lanzando tarea con OFFSET {offset}\")\n",
    "            future = executor.submit(get_data_batch_direct, offset, batch_size)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            all_data.extend(result)\n",
    "    print(f\"Extracción con hilos tomó: {time.time() - inicio:.2f} segundos\")\n",
    "\n",
    "    # Paso 6: Retornar todos los datos extraídos\n",
    "    return all_data\n",
    "\n",
    "def get_data_batch_direct(offset, limit):\n",
    "    cnx = None\n",
    "    try:\n",
    "        cnx = get_connection()\n",
    "        cursor = cnx.cursor()\n",
    "        cursor.execute(f\"SELECT * FROM UN.VENTAS LIMIT {limit} OFFSET {offset}\")\n",
    "        data = cursor.fetchall()\n",
    "        cursor.close()\n",
    "        return data\n",
    "    finally:\n",
    "        if cnx:\n",
    "            cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import time\n",
    "import mysql.connector\n",
    "dask.config.set(scheduler='threads', num_workers=5)\n",
    "# Total de filas\n",
    "def get_total_rows():\n",
    "    cnx = get_connection()\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM UN.VENTAS\")\n",
    "    total = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    return total\n",
    "\n",
    "# Tarea diferida\n",
    "@delayed\n",
    "def get_data_batch(offset, limit):\n",
    "    cnx = get_connection()\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(f\"SELECT * FROM UN.VENTAS LIMIT {limit} OFFSET {offset}\")\n",
    "    data = cursor.fetchall()\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Extracción principal\n",
    "def extract_all_data_dask(batch_size):\n",
    "    total_rows = get_total_rows()\n",
    "    offsets = list(range(0, total_rows, batch_size))  # solo 5 si batch_size=200_000\n",
    "    \n",
    "    print(f\"Lanzando {len(offsets)} tareas con Dask\")\n",
    "    inicio = time.time()\n",
    "\n",
    "    tasks = [get_data_batch(offset, batch_size) for offset in offsets]\n",
    "    result_dfs = dask.compute(*tasks)  # Ejecuta en paralelo\n",
    "\n",
    "    final_df = pd.concat(result_dfs, ignore_index=True)\n",
    "\n",
    "    print(f\"Extracción con Dask tomó: {time.time() - inicio:.2f} segundos\")\n",
    "    return final_df\n",
    "\n",
    "# Lanzamiento\n",
    "df = extract_all_data_dask(batch_size=200_000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ae19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar y medir el tiempo para cada formato\n",
    "def measure_export_times(df, max_n):\n",
    "    export_times = {\n",
    "        \"CSV\": [],\n",
    "        \"JSON\": [],\n",
    "        \"Parquet\": [],\n",
    "        \"Avro\": []  # Placeholder, Avro requiere implementación con fastavro o avro-python3\n",
    "    }\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        sample = df.iloc[:n]\n",
    "\n",
    "        # CSV\n",
    "        start = time.time()\n",
    "        sample.to_csv(\"temp.csv\", index=False)\n",
    "        export_times[\"CSV\"].append((n, time.time() - start))\n",
    "\n",
    "        # JSON\n",
    "        start = time.time()\n",
    "        sample.to_json(\"temp.json\", orient=\"records\", lines=True)\n",
    "        export_times[\"JSON\"].append((n, time.time() - start))\n",
    "\n",
    "        # Parquet\n",
    "        start = time.time()\n",
    "        sample.to_parquet(\"temp.parquet\")\n",
    "        export_times[\"Parquet\"].append((n, time.time() - start))\n",
    "\n",
    "        # Avro (opcional: debes implementar si deseas usarlo)\n",
    "\n",
    "    return export_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d55017",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extrayendo datos...\")\n",
    "inicio = time.time()\n",
    "data = extract_all_data_parallel(200000, 5)\n",
    "print(time.time() - inicio)\n",
    "\n",
    "\n",
    "print(f\"Total de registros extraídos: {len(data)}\")\n",
    "\n",
    "columns = ['ID_VENTA', 'FECHA_VENTA', 'ID_CLIENTE', 'ID_EMPLEADO',\n",
    "            'ID_PRODUCTO', 'CANTIDAD', 'PRECIO_UNITARIO', 'DESCUENTO', 'FORMA_PAGO']\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID_VENTA', 'FECHA_VENTA', 'ID_CLIENTE', 'ID_EMPLEADO',\n",
    "                  'ID_PRODUCTO', 'CANTIDAD', 'PRECIO_UNITARIO', 'DESCUENTO', 'FORMA_PAGO'])\n",
    "\n",
    "df\n",
    "\n",
    "\"\"\"print(\"Midiendo tiempos de exportación...\")\n",
    "\n",
    "#export_results = measure_export_times(df, max_n=10000)  # o hasta 1_000_000 según tu capacidad\n",
    "\n",
    "print(\"Exportación terminada.\")\n",
    "\n",
    "\n",
    "\n",
    "for fmt, times in export_results.items():\n",
    "    print(f\"\\nFormato: {fmt}\")\n",
    "    for n, t in times[:5]:  # muestra solo los primeros 5 para verificar\n",
    "        print(f\"{n} registros: {t:.4f} s\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      " config:  {'user': 'ed2studentsG6', 'password': 'd8d0274ccb39fcded4a5fac1961580104770fa93620f4a699cccecae58e8efe5', 'host': 'lab-ed.c9q48as80ych.us-east-1.rds.amazonaws.com', 'database': 'UN', 'charset': 'utf8'}\n",
      "Connection established\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_VENTA</th>\n",
       "      <th>FECHA_VENTA</th>\n",
       "      <th>ID_CLIENTE</th>\n",
       "      <th>ID_EMPLEADO</th>\n",
       "      <th>ID_PRODUCTO</th>\n",
       "      <th>CANTIDAD</th>\n",
       "      <th>PRECIO_UNITARIO</th>\n",
       "      <th>DESCUENTO</th>\n",
       "      <th>FORMA_PAGO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00004df7-75ba-4693-b8a8-e7157fca6b35</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>5476</td>\n",
       "      <td>810</td>\n",
       "      <td>1178</td>\n",
       "      <td>13</td>\n",
       "      <td>666.58</td>\n",
       "      <td>20.75</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00006952-8e9e-4f4a-871b-0c8a905d837c</td>\n",
       "      <td>2024-08-10</td>\n",
       "      <td>942</td>\n",
       "      <td>730</td>\n",
       "      <td>1577</td>\n",
       "      <td>16</td>\n",
       "      <td>259.38</td>\n",
       "      <td>39.74</td>\n",
       "      <td>Check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00006a1a-5bfd-4491-aec3-1bff92e28a77</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>7299</td>\n",
       "      <td>339</td>\n",
       "      <td>456</td>\n",
       "      <td>8</td>\n",
       "      <td>617.91</td>\n",
       "      <td>0.27</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00006cc9-bee2-4b74-b32b-7a4d4ae0f175</td>\n",
       "      <td>2024-09-29</td>\n",
       "      <td>997</td>\n",
       "      <td>252</td>\n",
       "      <td>1050</td>\n",
       "      <td>14</td>\n",
       "      <td>829.02</td>\n",
       "      <td>43.88</td>\n",
       "      <td>Check</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007785-74c5-4e49-910e-415b2f1d9ae6</td>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>7284</td>\n",
       "      <td>265</td>\n",
       "      <td>1260</td>\n",
       "      <td>4</td>\n",
       "      <td>407.57</td>\n",
       "      <td>8.31</td>\n",
       "      <td>Cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00007f45-6011-40d8-ac7e-22154196994f</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>2159</td>\n",
       "      <td>804</td>\n",
       "      <td>3892</td>\n",
       "      <td>6</td>\n",
       "      <td>608.28</td>\n",
       "      <td>2.36</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00008479-a90a-419e-b9ae-2d2c897a94f1</td>\n",
       "      <td>2025-03-24</td>\n",
       "      <td>7381</td>\n",
       "      <td>201</td>\n",
       "      <td>3182</td>\n",
       "      <td>2</td>\n",
       "      <td>476.49</td>\n",
       "      <td>2.76</td>\n",
       "      <td>Cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00008dcb-c5dc-4f99-8524-ad1ee76b0545</td>\n",
       "      <td>2024-10-25</td>\n",
       "      <td>163</td>\n",
       "      <td>449</td>\n",
       "      <td>1020</td>\n",
       "      <td>11</td>\n",
       "      <td>650.25</td>\n",
       "      <td>32.61</td>\n",
       "      <td>Cash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000094d2-4319-47eb-b5a0-6637883546d9</td>\n",
       "      <td>2024-09-12</td>\n",
       "      <td>1688</td>\n",
       "      <td>679</td>\n",
       "      <td>1553</td>\n",
       "      <td>7</td>\n",
       "      <td>759.96</td>\n",
       "      <td>10.27</td>\n",
       "      <td>Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0000ace5-cb0b-4f33-b9b1-01d2be779ab9</td>\n",
       "      <td>2024-08-17</td>\n",
       "      <td>5800</td>\n",
       "      <td>329</td>\n",
       "      <td>3645</td>\n",
       "      <td>4</td>\n",
       "      <td>3.16</td>\n",
       "      <td>8.05</td>\n",
       "      <td>Transfer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               ID_VENTA FECHA_VENTA  ID_CLIENTE  ID_EMPLEADO  \\\n",
       "0  00004df7-75ba-4693-b8a8-e7157fca6b35  2024-08-09        5476          810   \n",
       "1  00006952-8e9e-4f4a-871b-0c8a905d837c  2024-08-10         942          730   \n",
       "2  00006a1a-5bfd-4491-aec3-1bff92e28a77  2024-06-30        7299          339   \n",
       "3  00006cc9-bee2-4b74-b32b-7a4d4ae0f175  2024-09-29         997          252   \n",
       "4  00007785-74c5-4e49-910e-415b2f1d9ae6  2024-09-12        7284          265   \n",
       "5  00007f45-6011-40d8-ac7e-22154196994f  2024-06-28        2159          804   \n",
       "6  00008479-a90a-419e-b9ae-2d2c897a94f1  2025-03-24        7381          201   \n",
       "7  00008dcb-c5dc-4f99-8524-ad1ee76b0545  2024-10-25         163          449   \n",
       "8  000094d2-4319-47eb-b5a0-6637883546d9  2024-09-12        1688          679   \n",
       "9  0000ace5-cb0b-4f33-b9b1-01d2be779ab9  2024-08-17        5800          329   \n",
       "\n",
       "   ID_PRODUCTO  CANTIDAD PRECIO_UNITARIO DESCUENTO FORMA_PAGO  \n",
       "0         1178        13          666.58     20.75   Transfer  \n",
       "1         1577        16          259.38     39.74      Check  \n",
       "2          456         8          617.91      0.27   Transfer  \n",
       "3         1050        14          829.02     43.88      Check  \n",
       "4         1260         4          407.57      8.31       Cash  \n",
       "5         3892         6          608.28      2.36   Transfer  \n",
       "6         3182         2          476.49      2.76       Cash  \n",
       "7         1020        11          650.25     32.61       Cash  \n",
       "8         1553         7          759.96     10.27       Card  \n",
       "9         3645         4            3.16      8.05   Transfer  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_connection1():\n",
    "    try:\n",
    "        print(\"Connecting to the database...\\n config: \", config)\n",
    "        return connect(**config)\n",
    "    except Error as err:\n",
    "        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with your user name or password\")\n",
    "        elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "        else:\n",
    "            print(err)\n",
    "        return None\n",
    "    \n",
    "cnx = get_connection1()\n",
    "\n",
    "print(\"Connection established\")\n",
    "\n",
    "data = get_data(cnx, \"SELECT * FROM UN.VENTAS LIMIT 10\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID_VENTA', 'FECHA_VENTA', 'ID_CLIENTE', 'ID_EMPLEADO',\n",
    "                  'ID_PRODUCTO', 'CANTIDAD', 'PRECIO_UNITARIO', 'DESCUENTO', 'FORMA_PAGO'])\n",
    "\n",
    "cnx.close()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dd96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_id_venta():\n",
    "    cnx = get_connection()\n",
    "    if not cnx:\n",
    "        return None, None\n",
    "    cursor = cnx.cursor()\n",
    "    cursor.execute(\"SELECT MIN(ID_VENTA), MAX(ID_VENTA) FROM UN.VENTAS\")\n",
    "    min_id, max_id = cursor.fetchone()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    return int(min_id), int(max_id)\n",
    "\n",
    "def get_data_by_id_venta_range(min_id, max_id):\n",
    "    cnx = get_connection()\n",
    "    if not cnx:\n",
    "        return []\n",
    "    cursor = cnx.cursor()\n",
    "    query = (\n",
    "        \"SELECT * FROM UN.VENTAS \"\n",
    "        f\"WHERE ID_VENTA >= {min_id} AND ID_VENTA < {max_id}\"\n",
    "    )\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    cnx.close()\n",
    "    return data\n",
    "\n",
    "def extract_all_by_idventa_in_parallel(num_threads):\n",
    "    min_id, max_id = get_min_max_id_venta()\n",
    "    if min_id is None or max_id is None:\n",
    "        print(\"No se pudo obtener el rango de ID_VENTA\")\n",
    "        return []\n",
    "\n",
    "    step = (max_id - min_id) // num_threads\n",
    "    ranges = [(min_id + i * step, min_id + (i + 1) * step) for i in range(num_threads - 1)]\n",
    "    ranges.append((min_id + (num_threads - 1) * step, max_id + 1))\n",
    "\n",
    "    print(\"División de rangos:\", ranges)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    def task(id_range):\n",
    "        print(f\"Leyendo ID_VENTA entre {id_range[0]} y {id_range[1]}\")\n",
    "        return get_data_by_id_venta_range(id_range[0], id_range[1])\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = [executor.submit(task, r) for r in ranges]\n",
    "        for f in futures:\n",
    "            all_data.extend(f.result())\n",
    "\n",
    "    return all_data\n",
    "\n",
    "cnx = get_connection()\n",
    "\n",
    "print(\"Connection established\")\n",
    "\n",
    "data = extract_all_by_idventa_in_parallel(5)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['ID_VENTA', 'FECHA_VENTA', 'ID_CLIENTE', 'ID_EMPLEADO',\n",
    "                  'ID_PRODUCTO', 'CANTIDAD', 'PRECIO_UNITARIO', 'DESCUENTO', 'FORMA_PAGO'])\n",
    "\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from time import sleep\n",
    "\n",
    "connections = []\n",
    "\n",
    "try:\n",
    "    for i in range(200):  # Intenta abrir hasta 200 conexiones\n",
    "        print(f\"Abrir conexión #{i}\")\n",
    "        cnx = mysql.connector.connect(**config)\n",
    "        connections.append(cnx)\n",
    "        sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(\"Error al abrir conexión:\", e)\n",
    "finally:\n",
    "    for cnx in connections:\n",
    "        cnx.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c826535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database...\n",
      " config:  {'user': 'ed2studentsG6', 'password': 'd8d0274ccb39fcded4a5fac1961580104770fa93620f4a699cccecae58e8efe5', 'host': 'lab-ed.c9q48as80ych.us-east-1.rds.amazonaws.com', 'database': 'UN', 'charset': 'utf8'}\n",
      "Connection established\n",
      "Se obtuvieron todos los datos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/resource_tracker.py:104: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 1] Operation not permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m sizes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1_000_000\u001b[39m, num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    115\u001b[0m sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(sizes)))  \u001b[38;5;66;03m# Eliminar posibles duplicados\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m results \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mdict({fmt: manager\u001b[38;5;241m.\u001b[39mlist() \u001b[38;5;28;01mfor\u001b[39;00m fmt \u001b[38;5;129;01min\u001b[39;00m formats})\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# ⚙️ Medición con multiprocessing\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/context.py:57\u001b[0m, in \u001b[0;36mBaseContext.Manager\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanagers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SyncManager\n\u001b[1;32m     56\u001b[0m m \u001b[38;5;241m=\u001b[39m SyncManager(ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_context())\n\u001b[0;32m---> 57\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/managers.py:562\u001b[0m, in \u001b[0;36mBaseManager.start\u001b[0;34m(self, initializer, initargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m ident \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39m_identity)\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m  \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m ident\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# get address of server\u001b[39;00m\n\u001b[1;32m    565\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/popen_spawn_posix.py:42\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     40\u001b[0m tracker_fd \u001b[38;5;241m=\u001b[39m resource_tracker\u001b[38;5;241m.\u001b[39mgetfd()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds\u001b[38;5;241m.\u001b[39mappend(tracker_fd)\n\u001b[0;32m---> 42\u001b[0m prep_data \u001b[38;5;241m=\u001b[39m \u001b[43mspawn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_preparation_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m     44\u001b[0m set_spawning_popen(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ed2/lib/python3.10/multiprocessing/spawn.py:176\u001b[0m, in \u001b[0;36mget_preparation_data\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     sys_path[i] \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mORIGINAL_DIR\n\u001b[1;32m    171\u001b[0m d\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    172\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    173\u001b[0m     sys_path\u001b[38;5;241m=\u001b[39msys_path,\n\u001b[1;32m    174\u001b[0m     sys_argv\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39margv,\n\u001b[1;32m    175\u001b[0m     orig_dir\u001b[38;5;241m=\u001b[39mprocess\u001b[38;5;241m.\u001b[39mORIGINAL_DIR,\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    177\u001b[0m     start_method\u001b[38;5;241m=\u001b[39mget_start_method(),\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Figure out whether to initialise main in the subprocess as a module\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# or through direct execution (or to leave it alone entirely)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m main_module \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 1] Operation not permitted"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from fastavro import writer, parse_schema\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from decimal import Decimal\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Manager\n",
    "from mysql.connector import connect, Error, errorcode\n",
    "\n",
    "formats = ['json', 'csv', 'parquet', 'avro']\n",
    "os.makedirs(\"output_files\", exist_ok=True)\n",
    "\n",
    "def get_connection():\n",
    "    try:\n",
    "        print(\"Connecting to the database...\\n config: \", config)\n",
    "        return connect(**config)\n",
    "    except Error as err:\n",
    "        if err.errno == errorcode.ER_ACCESS_DENIED_ERROR:\n",
    "            print(\"Something is wrong with your user name or password\")\n",
    "        elif err.errno == errorcode.ER_BAD_DB_ERROR:\n",
    "            print(\"Database does not exist\")\n",
    "        else:\n",
    "            print(err)\n",
    "        return None\n",
    "\n",
    "def get_data(connection: connect, query: str):\n",
    "    my_cursor = connection.cursor()\n",
    "    my_cursor.execute(query)\n",
    "    data = my_cursor.fetchall()\n",
    "    my_cursor.close()\n",
    "    return data\n",
    "\n",
    "def serialize_data(df, fmt, filename):\n",
    "    path = os.path.join(\"output_files\", filename)\n",
    "\n",
    "    if fmt == 'json':\n",
    "        df = df.copy()\n",
    "        for col in df.columns:\n",
    "            if df[col].apply(lambda x: isinstance(x, (datetime.date, datetime.datetime))).any():\n",
    "                df[col] = df[col].astype(str)\n",
    "        df.to_json(path, orient='records', lines=True)\n",
    "\n",
    "    elif fmt == 'csv':\n",
    "        df.to_csv(path, index=False)\n",
    "\n",
    "    elif fmt == 'parquet':\n",
    "        table = pa.Table.from_pandas(df)\n",
    "        pq.write_table(table, path)\n",
    "\n",
    "    elif fmt == 'avro':\n",
    "        df = df.copy()\n",
    "        for col in df.columns:\n",
    "            if df[col].apply(lambda x: isinstance(x, (datetime.date, datetime.datetime))).any():\n",
    "                df[col] = df[col].astype(str)\n",
    "            elif df[col].apply(lambda x: isinstance(x, Decimal)).any():\n",
    "                df[col] = df[col].astype(float)\n",
    "\n",
    "        records = df.to_dict(orient='records')\n",
    "        avro_types = {\n",
    "            'int64': 'long',\n",
    "            'float64': 'double',\n",
    "            'object': 'string',\n",
    "            'bool': 'boolean'\n",
    "        }\n",
    "\n",
    "        schema = {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"Venta\",\n",
    "            \"fields\": []\n",
    "        }\n",
    "\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            avro_type = avro_types.get(dtype, 'string')\n",
    "            schema[\"fields\"].append({\n",
    "                \"name\": col,\n",
    "                \"type\": [\"null\", avro_type],\n",
    "                \"default\": None\n",
    "            })\n",
    "\n",
    "        with open(path, 'wb') as out:\n",
    "            writer(out, parse_schema(schema), records)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Formato no soportado\")\n",
    "\n",
    "def medir_tiempo_promedio(fmt, df_sample, size, results):\n",
    "    tiempos = []\n",
    "    for _ in range(10):\n",
    "        filename = f\"{fmt}_{size}.{'avro' if fmt == 'avro' else fmt}\"\n",
    "        start = time.time()\n",
    "        serialize_data(df_sample.copy(), fmt, filename)\n",
    "        tiempos.append(time.time() - start)\n",
    "\n",
    "    promedio = sum(tiempos) / len(tiempos)\n",
    "    results[fmt].append(promedio)\n",
    "    print(f\"[{fmt.upper()}] Tamaño: {size} → Promedio: {promedio:.4f} s\")\n",
    "\n",
    "# 🚀 Iniciar conexión y cargar datos\n",
    "cnx = get_connection()\n",
    "print(\"Connection established\")\n",
    "data = get_data(cnx, \"SELECT * FROM UN.VENTAS\")\n",
    "df_full = pd.DataFrame(data, columns=['ID_VENTA', 'FECHA_VENTA', 'ID_CLIENTE', 'ID_EMPLEADO',\n",
    "                  'ID_PRODUCTO', 'CANTIDAD', 'PRECIO_UNITARIO', 'DESCUENTO', 'FORMA_PAGO'])\n",
    "cnx.close()\n",
    "print(\"Se obtuvieron todos los datos\")\n",
    "\n",
    "# 🆕 Tamaños de muestra\n",
    "sizes = np.linspace(100, 1_000_000, num=30, dtype=int)\n",
    "sizes = sorted(list(set(sizes)))  # Eliminar posibles duplicados\n",
    "\n",
    "manager = Manager()\n",
    "results = manager.dict({fmt: manager.list() for fmt in formats})\n",
    "\n",
    "# ⚙️ Medición con multiprocessing\n",
    "for size in sizes:\n",
    "    if size > len(df_full):\n",
    "        print(f\"❌ Saltando tamaño {size}, no hay suficientes datos.\")\n",
    "        continue\n",
    "\n",
    "    df_sample = df_full.head(size)\n",
    "    print(f\"\\n📦 Procesando tamaño: {size} registros\")\n",
    "\n",
    "    processes = []\n",
    "    for fmt in formats:\n",
    "        p = Process(target=medir_tiempo_promedio, args=(fmt, df_sample, size, results))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "# ✅ Convertir a diccionario regular para graficar\n",
    "results_dict = {fmt: list(results[fmt]) for fmt in formats}\n",
    "\n",
    "# 📊 Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "for fmt in formats:\n",
    "    plt.plot(sizes[:len(results_dict[fmt])], results_dict[fmt], marker='o', label=fmt.upper())\n",
    "\n",
    "plt.xlabel(\"Cantidad de registros\")\n",
    "plt.ylabel(\"Tiempo promedio de escritura (s)\")\n",
    "plt.title(\"Comparación de formatos de serialización (multiprocessing, 10 repeticiones)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ed2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
